Accomplish in a language of your choice:

Input: Given an array of integers

Output: In whatever representation you wish, output each integer in the array and all the other integers in the array that are
factors of the first integer.

Example:

  Given an array of [10, 5, 2, 20], the output would be:

{10: [5, 2], 5: [], 2: [], 20: [10,5,2]}

Additional Questions:

1.  What if you were to cache the calculation, for example in the file system.  What would an example implementation
of the cache look like?  By cache I mean, given an array input, skip the calculation of the output if you have already
calculated the output at least once already.

Caching strategies are an inherantly application specific problem. The profile of how your application accesses the cache should determine the style of cache you should use.

For example, a cache where the normal usage is for a small subset of the cache entries to be accessed frequently benefits most from techniques that keep those values in memory (probably in a hash table) or in a seperate frequently used cache. Incoming requests can be checked against that cache before checking in the larger (most likely) slower cache. A usage pattern where values are accessed uniformly would not benefit from this optimization and should instead focus on general lookup speed.

I'm going to focus my designs on file system based solutions but they should also work in memory but with different pointer semantics.

Both solutions are going to depend on having a file based heap allocator we'll call DataHeap.

# DataHeap serves a file based heap allocator
# It will keep track of what parts of the file have been allocated and what parts are still free. It'll add a little bit of overhead with headers but it'll pay for itself in being able to allocate more efficently.
# The headers will act as a linked list which can be walked to find the free parts of the file.
# Initially there will be one node in the list which contains the whole file.
class DataHeap
  # The allocator will create a file of this size to do its initial allocations
  def initialize(file, size)

  # The allocator will attempt to extend the size of the file
  # An alternative implementation would be to add additional datafiles, turning the location pointer into a combination {file: file_ptr, location: file_location_ptr}
  def extend(size)

  # This will carve a new block out of the file.
  # There are a number of algorithims for doing this but the simplest would involve giving each block allocation a short header which includes size, next block location, and allocation status (allocated, true or false)
  # The allocation algorithim could walk the list of blocks until it finds the first one that is free and the data fits in and return a pointer to the data portion of the block to the user while also updating the header (and creating a new block in the chain if it took only a portion of the block)
  # A more complicated algorithim would involve chunking by allocation size and potentially fixed sized blocks
  def allocate(size)

  # This would return a block to the free pool.
  # The deallocater can use the location to find the header and change the block to unallocated. It could also look backwards and forwards in the block chain to determine if it can coalace the block into its neighbors
  def delete(location)
end

The first caching solution will be a HashTable.

# HashCache

For HashCache we are going to go the straight forward route of just building a hashtable of the inputs.

Assumptions:
* We are going to sort the input arrays before hashing to cut down on repeats

Pros:
* It should have relatively fast lookup times since we will be able to use the hashing algorithim to find the data location in one step
* The naieve implementation is straight forward

Cons:
* Disk space will be used inefficently
* Resizing the cache will be an expensive operation, potentially impossible after a certain percentage of disk space used
* The optimized implementation will require implementing a memory management strategy

The Design:

# This is a basic file based hashtable
# We are going to treat the file as a giant array with fixed size buckets.
# The buckets are going to contain pointers to locations in the DataHeap.
# We are going to handle collisions by having each entry in the DataHeap be a List.
class HashTable
  # We will allocate a file of size and keep a reference to a DataHeap
  def initialize(file, size, data_heap)

  # Hash the key and find the location in the array
  # Look to see what is at that location, if it is null then allocate enough space for the data off of the Heap and put the data there as a one element list
  # If it is a pointer then get the data from the Heap location
  # Check each item of the data (it'll be a list) to see if the data matches, if so do nothing and return
  # If not append the data to the list and delete the location from the heap, allocate a new location on the Heap for the new data size and put the data there
  def put(key, data)

  # Hash the key and find the location in the array
  # Look to see what is at that location, if it is null then return null
  # If it is a pointer then get the data from the Heap location
  # Check each item of the data (it'll be a list) to see if any of the data matches the key (check to see if the keys and array are equal), if so return that item
  # If not return null
  def get(key)
end

Caching strategies (or how do we get rid of the extra data):

Depending on usage we could easily implement one of these strategies for dealing with too much data, depending on our usage patterns.

1. If we have few frequently accessed keys and many infrequently accessed keys we can add an additional step to track usage. When the cache gets too full (at whatever threshold), we can walk the hash entries and delete all of the ones that are infrequently used.
2. If we have a more uniform usage pattern (or we don't want to track usage), we can drop entries randomly once the cache gets too full. This could prove painful if we delete a computationally expensive key.
3. We can delete entries based on a differant weighting mechanism, such as computational cost to regenerate (delete the easy stuff), or size (delete the big DataHeap entries).

# TrieCache

It is possible to compose a cache based on a data structure called a Trie (https://en.wikipedia.org/wiki/Trie). This is because of an attribute of the problem once the input data is sorted.

factorize([2, 4]) => {2: [], 4: [2]}
factorize([2, 4, 8]) => factorize([2, 4]) + {8: [2, 4]}
factorize([2, 4, 9]) => factorize([2, 4]) + {9: []}

As you walk through a sorted array in this problem larger numbers never affect the solution for smaller numbers.

Each node in the TrieCache would contain two elements:

factor_data # This would be like the {8: [2,4]} in the previous example
next_numbers # This would be pointers to nodes for 8 and 9 in the previous example

next_numbers would not be shared between different arms in the tree.

Pros:
* This cache could provide useful output for the use of solving future factorization problems
* Pruning strategies can be highly effective because of building on the sub problem solving aspect
* It should be more efficient in disk space usage than than a hash table
* You get to implement a Trie!

Cons:
* Highly disparate datasets will involve long node walks to no advantage
* Your lookup time will be O(n) instead of a hash function O(1) (depending on hashing algorithim and collisions)
* If your disk(s) are slow then this will be very slow due to lots of disk accesses.

The Design:

class TrieNode
  # It should allocate some space on the heap for its instance data (number, factors) and space for future node references
  def initialize(number, factors, data_heap)

  # It should pop itself off the key (key should be a sorted array of numbers) and solution (hash of numbers to factors) (based on the node's number)
  # If it already contains the number in its next_numbers hash it should call add_node with the key and solution
  # If it doesn't then it should initialize a new node and store the reference, then it should call add_node with the key and solution
  def add_node(key, solution)

  # It should pop itself off the key and add its number entry to the solution
  # If it has a refence to the next number in its next_numbers hash, it should call find with the key and solution
  # If it doesn't have a reference it should return the partial solution, key combination
  # If key is empty, it should return the solution
  def find(key, solution)
end

Caching strategies (or how do we get rid of the extra data):

Depending on usage we could easily implement one of these strategies for dealing with too much data, depending on our usage patterns.

1. We could add a counter to each node that is increamented each access. If we want to prune the tree we could drop all nodes with low access counts. This along with utilizing partial solutions in future calcuations would provide a significant improvement in calcuation time even in the case of cache misses.
2. Along the same lines we could just drop all nodes past a certain length.

####

2.  What is the performance of your caching implementation?  Is there any way to make it more performant.

# HashCache

The worst case is that all of the keys hash to the same value and you end up just searching an array O(n) (n in this case being hash entry count)

Assuming few conflicts, lookup and allocation could be as fast as O(1).

The performance challenges with this solution end up involving the large files that are needed to store the hash table (with few conflicts) and the Heap. Disk accesses will be fairly random which will not help performance. SSDs will go a long way to make this solution faster.

One other thing that will speed access is to place the Heap and HashTable on different disks (especially in the case of spinning disks since the table and heap will both be accessing somewhat randomly).

# TrieCache

We will need to walk the entire array in every lookup. For long arrays this could involve many disk accesses which could be very expensive. The node structure will consume extra disk space, we will save space over a hashtable if we have many entries that share common roots.

To improve performance we could roll up long non-branching parts of the tree into single nodes. We could also keep track of multiple root nodes with different prefixes if we saw them often (a cache for the cache!) (data access patterns determine optimizations).

SSDs and RAM disks would provide huge value in making the random disk accesses fast.

For extremely long and large datasets you could create nodes that would move a particular find command over to a different server (this server has all the solutions for arrays that start with [2,3,5]).

Depending on performance needs, you could have search agents walk the tree together until they diverge, saving on disk access calls.

####

3.  What if you wanted to reverse the functionality.  What if you wanted to output each integer and all the other integers in the
array that is the first integer is a factor of I.E:

Given an array of [10, 5, 2, 20], the output would be:
{10: [20], 5: [10,20], 2: [10, 20], 20: []}

Would this change your caching algorithim?

# HashCache

The hashtable wouldn't need any changes. The check function for the data lists to the keys is based on the array entries being keys in the data hashes.

# TrieCache

It would not require any changes. You would sort the array in the opposite direction on input.

